Write-through cache directs write I/O onto cache and through to underlying permanent storage before confirming I/O completion to the host.
This ensures data updates are safely stored on, for example, a shared storage array, but has the disadvantage that I/O still experiences
latency based on writing to that storage. Write-through cache is good for applications that write and then re-read data frequently as data is
stored in cache and results in low read latency.

Write-around cache is a similar technique to write-through cache, but write I/O is written directly to permanent storage, bypassing the cache.
This can reduce the cache being flooded with write I/O that will not subsequently be re-read, but has the disadvantage is that a read request for
recently written data will create a “cache miss” and have to be read from slower bulk storage and experience higher latency.

Write-back cache is where write I/O is directed to cache and completion is immediately confirmed to the host.
This results in low latency and high throughput for write-intensive applications, but there is data availability exposure risk because
the only copy of the written data is in cache. As we will discuss later, suppliers have added resiliency with products that duplicate writes.
Users need to consider whether write-back cache solutions offer enough protection as data is exposed until it is staged to external storage.
Write-back cache is the best performing solution for mixed workloads as both read and write I/O have similar response time levels.